{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "119d23fd",
   "metadata": {},
   "source": [
    "# Train and Finetune\n",
    "This section shows how to train and finetune EO-1 on libero and custom dataset. Detailed scripts can be found in [../experiments/2_libero](../experiments/2_libero/train.sh)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70246600",
   "metadata": {},
   "source": [
    "## 1. Download Libero Dataset and Qwen2.5-VL-3B-Instruct\n",
    "\n",
    "Before running the following code, please download the libero dataset from the [libero-benchmark-dataset](https://huggingface.co/collections/IPEC-COMMUNITY/libero-benchmark-dataset-684837af28d465aa8b043950) and Qwen2.5-VL-3B-Instruct model from the [Qwen2.5-VL-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct) with huggingface-cli.\n",
    "\n",
    "```bash\n",
    "# Install Hugging Face CLI if not already installed\n",
    "pip install huggingface-cli\n",
    "huggingface-cli login\n",
    "\n",
    "# Download libero dataset\n",
    "datasets=(\n",
    "    libero_spatial_no_noops_1.0.0_lerobot\n",
    "    libero_object_no_noops_1.0.0_lerobot\n",
    "    libero_90_no_noops_lerobot\n",
    "    libero_10_no_noops_1.0.0_lerobot\n",
    ")\n",
    "\n",
    "HF_LEROBOT_HOME=YOUR_PATH_TO_DATASET\n",
    "\n",
    "for dataset in ${datasets[@]};\n",
    "do\n",
    "  echo \"Downloading ${dataset}...\"\n",
    "  huggingface-cli download \\\n",
    "  --repo-type dataset --resume-download --local-dir-use-symlinks False \\\n",
    "  IPEC-COMMUNITY/${dataset} \\\n",
    "  --local-dir ${HF_LEROBOT_HOME}/${dataset}\n",
    "done\n",
    "```\n",
    "\n",
    "Download the Qwen2.5-VL-3B-Instruct model from the [Qwen2.5-VL-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct) with huggingface-cli.\n",
    "\n",
    "```bash\n",
    "huggingface-cli download \\\n",
    "  --resume-download --local-dir-use-symlinks False \\\n",
    "  Qwen/Qwen2.5-VL-3B-Instruct \\\n",
    "  --local-dir ../pretrained/Qwen2.5-VL-3B-Instruct\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c387a2df",
   "metadata": {},
   "source": [
    "## 2. Finetune on Libero Dataset\n",
    "\n",
    "Set the dataset config in `experiments/2_libero/data-libero.yaml` according to the metadata `info.json` in the dataset:\n",
    "\n",
    "```yaml\n",
    "lerobot_datasets:\n",
    "  - repo_id: libero_spatial_no_noops_1.0.0_lerobot\n",
    "    root: HF_LEROBOT_HOME\n",
    "    select_video_keys: [observation.images.image, observation.images.wrist_image]\n",
    "    select_state_keys: [observation.state]\n",
    "    select_action_keys: [action]\n",
    "\n",
    "  - repo_id: libero_90_no_noops_lerobot\n",
    "    root: HF_LEROBOT_HOME\n",
    "    select_video_keys: [observation.images.image, observation.images.wrist_image]\n",
    "    select_state_keys: [observation.state]\n",
    "    select_action_keys: [action]\n",
    "\n",
    "  - repo_id: libero_object_no_noops_1.0.0_lerobot\n",
    "    root: HF_LEROBOT_HOME\n",
    "    select_video_keys: [observation.images.image, observation.images.wrist_image]\n",
    "    select_state_keys: [observation.state]\n",
    "    select_action_keys: [action]\n",
    "\n",
    "  - repo_id: libero_10_no_noops_1.0.0_lerobot\n",
    "    root: HF_LEROBOT_HOME\n",
    "    # automatically load all features if not specified\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d886bd0",
   "metadata": {},
   "source": [
    "Start training with the following command, and the model will be saved in `./outputs/libero_train`.\n",
    "\n",
    "```bash\n",
    "accelerate launch $ACCELERATE_ARGS scripts/train.py \\\n",
    "    --vlm-name-or-path ../pretrained/Qwen2.5-VL-3B-Instruct \\\n",
    "    --data-path experiments/2_libero/data-libero.yaml \\\n",
    "    --chunk-size 8 \\\n",
    "    --dataloader-num-workers 8 \\\n",
    "    --bf16 True \\\n",
    "    --tf32 True \\\n",
    "    --fp16 False \\\n",
    "    --num-train-epochs 50 \\\n",
    "    --per-device-train-batch-size 256 \\\n",
    "    --learning-rate 1e-4 \\\n",
    "    --merger-lr 1e-4 \\\n",
    "    --vision-lr 2e-5 \\\n",
    "    --weight-decay 0.1 \\\n",
    "    --warmup-ratio 0.03 \\\n",
    "    --lr-scheduler-type cosine \\\n",
    "    --gradient-checkpointing True \\\n",
    "    --save-strategy steps \\\n",
    "    --logging-steps 100 \\\n",
    "    --save-steps 5000 \\\n",
    "    --save-total-limit 3 \\\n",
    "    --report-to none \\\n",
    "    --run-name libero_train \\\n",
    "    --attn-implementation flash_attention_2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ab8bfe",
   "metadata": {},
   "source": [
    "## 3 Visualize the Trained Model\n",
    "\n",
    "Use the following command to visualize the trained model, where [../tools/openloop.py](../tools/openloop.py) read a lerobot dataset and visualize the inference action trajectory with the trained model.\n",
    "\n",
    "```bash\n",
    "python tools/openloop.py \\\n",
    "    --repo-id libero_spatial_no_noops_1.0.0_lerobot \\\n",
    "    --root HF_LEROBOT_HOME \\\n",
    "    --model_path ./outputs/libero_train/path/to/checkpoint\n",
    "```\n",
    "\n",
    "The script will visualize the inference action trajectory. With the following result:\n",
    "\n",
    "<img src=\"../.assets/openloop_example.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69841ee7",
   "metadata": {},
   "source": [
    "## 4 Finetune on Custom Dataset\n",
    "\n",
    "To fine-tune **EO-1** on your own embodiment, you only need to adapt the configuration file. Specifically, convert your dataset into the LeRobot format, then define the fields that describe where your videos, states, and actions are located.\n",
    "\n",
    "### 4.1 Dataset Conversion with Any4LeRobot\n",
    "\n",
    "[Any4LeRobot](https://github.com/Tavish9/any4lerobot) is a comprehensive tool collection for LeRobot that provides data conversion scripts, preprocessing tools, and training workflow helpers. Supported Input Formats\n",
    "\n",
    "- **Custom Video + State + Action**: Convert from custom data structures\n",
    "- **RLDS**: Convert from RLDS (Reinforcement Learning Datasets) format\n",
    "- **RoboSet**: Convert from RoboSet format\n",
    "- **Custom JSON**: Convert from custom JSON configurations\n",
    "\n",
    "Please refer to the [Any4LeRobot](https://github.com/Tavish9/any4lerobot), clone the repo, and select the corresponding format to convert your dataset.\n",
    "\n",
    "### 4.2 Dataset Configuration\n",
    "\n",
    "Once your dataset is converted to LeRobot format, create a configuration file (e.g., `custom_dataset.yaml`):\n",
    "\n",
    "```yaml\n",
    "# @multimodal data config\n",
    "# leave empty if only robot control data\n",
    "mm_datasets:\n",
    "\n",
    "lerobot_datasets:\n",
    "  - repo_id: your_custom_dataset_name  # replace with your dataset name\n",
    "    root: ./your_dataset_path/         # replace with your dataset root path\n",
    "    select_video_keys: [\n",
    "        observation.images.image,      # replace with your video feature keys\n",
    "        observation.images.wrist_image,\n",
    "      ]\n",
    "    select_state_keys: [observation.state]  # replace with your state keys\n",
    "    select_action_keys: [action]            # replace with your action keys\n",
    "    # Optional fields:\n",
    "    episodes: [1, 2, 3]                     # specific episodes to load (None = all)\n",
    "    train_subtask: mix:0.9                  # mix sub-task instructions and overall instructions\n",
    "    delta_action: false                     # train with delta actions\n",
    "    state_mode: \"MEAN_STD\"                  # state normalization mode\n",
    "    effector_indices: [14, 15]              # indices of effector channels\n",
    "    weight: 1.0                             # dataset weight for sampling\n",
    "\n",
    "  # Add more datasets if needed\n",
    "  - repo_id: another_dataset\n",
    "    root: ./another_dataset_path/\n",
    "    # If not specified, uses all keys by default\n",
    "```\n",
    "\n",
    "### 4.3 Training Configuration\n",
    "\n",
    "Create a training script (e.g., `train_custom.sh`) based on the Libero training script:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Set your custom dataset path\n",
    "CUSTOM_DATA_PATH=\"experiments/custom/data-custom.yaml\"\n",
    "OUTPUT_DIR=\"./outputs/custom_train\"\n",
    "\n",
    "# Training hyperparameters\n",
    "ACCELERATE_ARGS=\"--config_file accelerate_config.yaml\"\n",
    "VLM_PATH=\"../pretrained/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "# Launch training\n",
    "accelerate launch $ACCELERATE_ARGS scripts/train.py \\\n",
    "    --vlm-name-or-path $VLM_PATH \\\n",
    "    --data-path $CUSTOM_DATA_PATH \\\n",
    "    --chunk-size 8 \\\n",
    "    --dataloader-num-workers 8 \\\n",
    "    --bf16 True \\\n",
    "    --tf32 True \\\n",
    "    --fp16 False \\\n",
    "    --num-train-epochs 50 \\\n",
    "    --per-device-train-batch-size 256 \\\n",
    "    --learning-rate 1e-4 \\\n",
    "    --merger-lr 1e-4 \\\n",
    "    --vision-lr 2e-5 \\\n",
    "    --weight-decay 0.1 \\\n",
    "    --warmup-ratio 0.03 \\\n",
    "    --lr-scheduler-type cosine \\\n",
    "    --gradient-checkpointing True \\\n",
    "    --save-strategy steps \\\n",
    "    --logging-steps 100 \\\n",
    "    --save-steps 5000 \\\n",
    "    --save-total-limit 3 \\\n",
    "    --report-to none \\\n",
    "    --run-name custom_train \\\n",
    "    --attn-implementation flash_attention_2 \\\n",
    "    --output-dir $OUTPUT_DIR\n",
    "```\n",
    "\n",
    "### 4.6 Tips for Custom Datasets\n",
    "\n",
    "1. **Data Quality**: Ensure your dataset has consistent video frame rates and action frequencies\n",
    "2. **Feature Keys**: Verify that your `select_video_keys`, `select_state_keys`, and `select_action_keys` match your dataset's metadata\n",
    "3. **Episode Selection**: Use the `episodes` field to select specific episodes for training/testing\n",
    "4. **State Normalization**: Choose appropriate `state_mode` (MEAN_STD, MIN_MAX, or NONE) based on your data distribution\n",
    "5. **Memory Management**: Adjust `chunk_size` and `per-device-train-batch-size` based on your GPU memory\n",
    "\n",
    "### 4.7 Troubleshooting\n",
    "\n",
    "- **Data Loading Issues**: Check that your dataset follows LeRobot format and paths are correct\n",
    "- **Memory Errors**: Reduce batch size or chunk size\n",
    "- **Training Instability**: Adjust learning rates or add gradient clipping\n",
    "- **Poor Performance**: Verify data quality and feature selection\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
